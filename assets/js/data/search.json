[ { "title": "아나콘다 설치 및 설정 (TBU)", "url": "/posts/set-anaconda/", "categories": "Python, Anaconda", "tags": "tips, Python, anaconda", "date": "2022-05-23 15:30:00 +0900", "snippet": "1. 아나콘다 설치 https://www.anaconda.com/products/individual 설치파일 다운로드 설치 후 환경 변수 설정 C:\\Users\\kaier\\Anaconda3 C:\\Users\\kaier\\Anaconda3\\Library C:\\Users\\kaier\\Anaconda3\\Scripts 2. 가상 환경 생성 cmd 또는 Anaconda Prompt 실행 가상환경 생성 &gt; conda create -n torch_beginner python=3.9.0 -&gt; torch_beginner 이라는 이름의 가상환경이 생성되며, 파이썬 버전은 3.9.0 가상환경 활성화``` conda env list conda environments: #base e:\\Anaconda3torch_beginner e:\\Anaconda3\\envs\\torch_beginner conda activate torch_beginner``` 가상환경 삭제 &gt; conda env remove -n torch_beginner 커널 설치 (가상환경 활성화 상태에서) &gt; pip install ipykernel&gt; python -m ipykernel install --user -0name torch_beginner 파이토치 설치 &gt; conda install pytorch torchvision -c pytorch conda activate시 오류가 발생하면 https://kaiercorp.github.io/posts/conda-activate/ vs code를 사용한다면 https://kaiercorp.github.io/posts/vscode-anaconda/" }, { "title": "PyTorch 입문3", "url": "/posts/study-pytorch-beginner3/", "categories": "Machine Learning, PyTorch", "tags": "study, Machine Learning, PyTorch, Beginner", "date": "2022-05-18 14:30:00 +0900", "snippet": "[0] 참고자료 PyTorch로 시작하는 딥 러닝 입문 유원준 외 1명 딥러닝 파이토치 교과서 서지영[7] 인공 신경망21. 역전파 BackPropagation Sample Model 입력층, 은닉층1, 출력층 각 층은 2개의 뉴런 사용 은닉층과 출력층은 활성화 함수로 시그모이드 사용 순전파 Forward Propagation 은닉층 z1 = x1 * W1 + x2 * W2 = 0.30.1 + 0.250.2 = 0.08 z2 = x1 * W3 + x2 * W4 = 0.30.4 + 0.350.2 = 0.11 은닉층 시그모이드 h1 = sigmoid(z1) = 0.51998934 h2 = sigmoid(z2) = 0.52747230 출력층 z3 = h1 * W5 + h2 * W6 = h1 * 0.45 + h2 * 0.4 = 0.44498412 z4 = h1 * W7 + h2 * W8 = h1 * 0.7 + h2 * 0.6 = 0.68047592 은닉층 시그모이드 o1 = sigmoid(z3) = 0.60944600 o2 = sigmoid(z4) = 0.66384491 오차 E = 1/2 * (targeto1 - outputo1)^2 + 1/2 * (targeto2 - outputo2)^2 E = 1/2 * (0.4 - 0.60944600)^2 + 1/2 * (0.6 - 0.66384491)^2 = 0.02397190 역전파 Sample 출력층과 은닉층 사이의 가중치 W5, W6, W7, W8을 업데이트 한다. W5 업데이트 경사 하강법을 통해 업데이트 W5+ = W5 - α*(θE/θW5) θE/θW5 = θE/θo1 * θo1/θz3 * θz3/θW5 θE/θo1 Error 값을 o1에 대해 미분 θE/θo1 = 2 * 1/2 * (targeto1 - outputo1)^(2 - 1) * (-1) + 0 θE/θo1 = - (targeto1 - outputo1) = - (0.4 - 0.60944600) = 0.20944600 θo1/θz3 o1은 시그모이드 함수의 출력값 시그모이드 함수의 미분은 f(x)*(1 - f(x)). θo1/θz3 = o1 * (1 - o1) = 0.60944600 * (1 - 0.60944600) = 0.23802157 θz3/θW5 θz3/θW5 = h1 = 0.51998934 θE/θW5 = θE/θo1 * θo1/θz3 * θz3/θW5 0.20944600 * 0.23802157 * 0.51998934 = 0.02592286 W5+ = W5 - α*(θE/θW5) α 는 learning rate 0.5라고 가정 0.45 - 0.5 * 0.02592286 = 0.43703857 2. 비선형 활성화 함수 Activation function 입력을 받아 수학적 변환을 수행하고 출력을 생성하는 함수 ex) sigmoid, softmax 특징 비선형 함수 Nonlinear function 직선 1개로는 그릴 수 없는 함수 활성화함수로 선형 함수를 선택하고 layer를 추가하는 경우, 활성화 함수 f(x) = Wx 은닉층을 추가하면 y(x) = f((fx)) = W1W2x = kx 즉, 은닉층을 여러번 추가해도 차이가 없음 활성화함수로 선형 함수를 주는 경우는 가중치의 변화를 좀 더 주고 싶은 경우 일반적으로는 비선형함수를 선택함 시그모이드 함수와 기울기 소실 시그모이드의 출력값이 0 또는 1에 가까워지면, 그래프의 기울기가 0에 가까워짐 역전파 과정에서 기울기(미분)을 사용하게 되는데, 이 때 기울기가 0에 가까워지면 앞단에 전달되지 않음 따라서 시그모이드 함수를 은닉층에 사용할 때는 어려움이 있음 하이퍼볼릭탄젠트 함수 Hyperbolic tangent function 입력값을 -1과 1 사이로 변환함 양 끝 기울기가 0에 가까워지는 문제는 시그모이드와 같음 0을 중심으로 하기때문에 시그모이드 함수보다는 기울기 소실 증상이 적은 편임 렐루 함수 ReLU 음수를 입력하면 0, 양수를 입력하면 그대로 반환 f(x) = max(0, x) 특정 값에 수렴하지 않으므로 시그모이드보다 잘 작동함 시그모이드, 하이퍼볼릭탄젠트와 같이 연산이 필요한 것이 아니므로 연산 속도가 빠름 입력값이 음수면 기울기도 0이 되는 것이 단점 - dying ReLU Leaky ReLU 렐루를 보완하기 위한 방법 중 하나 입력값이 음수인 경우 0.001과 같은 매우 작은 수를 반환 Softmax function 출력층에 적용할 수 있음 [8] 합성곱 신경망 CNN Convolutional Neural Network1. Why Convolution? 이미지 데이터를 입력으로 주면 결국 1차원 데이터가 되는데, 변환 전의 공간적인 구조 정보가 유실됨 공간적인 구조 정보 : 거리가 가까운 픽셀간의 관계2. Channel 이미지는 (높이 x 너비 x 채널) 이라는 3차원 텐서 높이 : 세로 방향 픽셀 수 너비 : 가로 방향 픽셀 수 채널 : 색 성분. 흑백은 1채널, 일반적인 컬러는 3채널3. Convolution opertation 합성곱층은 이미지의 특징을 추출하는 역할 커널(kernel) 또는 필터(filter)라는 n x m 크기의 행렬로 이미지를 처음부터 끝까지 훑으면서 출력값을 생성함 커널은 일반적으로 3 x 3 또는 5 x 5를 사용함(1×1) + (2×0) + (3×1) + (2×1) + (1×0) + (0×1) + (3×0) + (0×1) + (1×0) = 6(2×1) + (3×0) + (4×1) + (1×1) + (0×0) + (1×1) + (0×0) + (1×1) + (1×0) = 9(3×1) + (4×0) + (5×1) + (0×1) + (1×0) + (2×1) + (1×0) + (1×1) + (0×0) = 11(2×1) + (1×0) + (0×1) + (3×1) + (0×0) + (1×1) + (1×0) + (4×1) + (1×0) = 10 입력과 커널의 합성곱으로 나온 결과를 특성 맵이라 함 커널의 크기, 이동범위(stride)는 사용자가 지정 가능 5 x 5 이미지에 3 x 3 커널 2스트라이드인 경우4. Padding 특성맵은 입력보다 크기가 작아진다는 단점이 있음 합성곱 층이 여러 개인 경우, 특성 맵의 크기가 너무 작아질 수 있음 합성곱 연산 전에 입력의 가장자리에 행과 열을 추가해주는 방법5. Multi-Channel Convolution 실제 합성곱 연산의 입력은 다수의 채널을 가진 이미지 또는 이전 연산의 결과일 수 있음 따라서 합성곱 연산시에는 커널의 채널 수 또한 입력 채널 수와 같아야 함6. CNN MNISTimport torchimport torchvision.datasets as dsetsimport torchvision.transforms as transformsimport torch.nn as nnimport torch.nn.initfrom torch.utils.data import DataLoaderdevice = 'cuda' if torch.cuda.is_available() else 'cpu'torch.manual_seed(777)if device == 'cuda': torch.cuda.manual_seed_all(777)learning_rate = 0.001trainint_epochs = 15batch_size = 100mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=True)mnist_test = dsets.MNIST(root='MNIST_data/', train=False, transform=transforms.ToTensor(), download=True)data_loader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() # 첫번째층 # ImgIn shape=(?, 28, 28, 1) # Conv -&gt; (?, 28, 28, 32) # Pool -&gt; (?, 14, 14, 32) self.layer1 = nn.Sequential( nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) # 두번째층 # ImgIn shape=(?, 14, 14, 32) # Conv -&gt;(?, 14, 14, 64) # Pool -&gt;(?, 7, 7, 64) self.layer2 = nn.Sequential( nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) # 전결합층 7x7x64 inputs -&gt; 10 outputs self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True) # 전결합층 한정으로 가중치 초기화 nn.init.xavier_uniform_(self.fc.weight) def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.view(out.size(0), -1) out = self.fc(out) return outmodel = CNN().to(device)criterion = nn.CrossEntropyLoss().to(device)optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)total_batch = len(data_loader)print('총 배치의 수 : {}'.format(total_batch))for epoch in range(trainint_epochs): avg_cost = 0 for X, Y in data_loader: # image is already size of (28 x 28), no reshpae # label is not one-hot encoded X = X.to(device) Y = Y.to(device) optimizer.zero_grad() hypothesis = model(X) cost = criterion(hypothesis, Y) cost.backward() optimizer.step() avg_cost += cost / total_batch print('[Epoch: {:&gt;4}] cost = {:&gt;.9}'.format(epoch + 1, avg_cost))with torch.no_grad(): X_test = mnist_test.data.view(len(mnist_test), 1, 28, 28).float().to(device) Y_test = mnist_test.targets.to(device) prediction = model(X_test) correct_prediction = model(X_test) correct_prediction = torch.argmax(prediction, 1) == Y_test accuracy = correct_prediction.float().mean() print('Accuracy: ', accuracy.item())" }, { "title": "PyTorch 입문2", "url": "/posts/study-pytorch-beginner2/", "categories": "Machine Learning, PyTorch", "tags": "study, Machine Learning, PyTorch, Beginner", "date": "2022-05-17 11:30:00 +0900", "snippet": "[0] 참고자료 PyTorch로 시작하는 딥 러닝 입문 유원준 외 1명 딥러닝 파이토치 교과서 서지영[5] 소프트맥스 회귀 Softmax Regression1. One-Hot Encodingwhat? 선택해야 하는 개수만큼의 차원을 가지면서, 각 선택지의 인덱스에 해당하는 원소에는 1, 나머지 원소는 0의 값을 가지도락 하는 방법강아지 = [1, 0, 0]고양이 = [0, 1, 0]앵무새 = [0, 0, 1]why?예를 들어 강아지 = 1, 고양이 = 2, 앵무새 = 3 라고 레이블을 주었을 때, 손실 함수로 MSE를 사용한다면,실제 값이 강아지 일 때 예측값이 고양이였다면 (2 - 1)^2 = 1,실제 값이 강아지 일 때 예측값이 앵무새였다면 (3 - 1)^2 = 4,즉, 강아지와 고양이가 가깝다는 정보를 주는 결과가 됨-&gt; 대부분의 분류 문제가 클래스 간의 관계가 균등하므로, 원-핫 인코딩을 많이 사용하게 됨2. 소프트맥스 회귀 로지스틱 회귀 소프트맥스 회귀-&gt; 소프트맥스 회귀는 선택지의 개수만큼의 차원을 갖는 벡터를 만들고, 벡터의 모든 합이 1이 되는 어떤 함수를 만들어야 함 가중치 계산-&gt; One-Hot 인코딩 된 실제값과 비교하여 Weight와 Bias를 업데이트 한다3. 비용함수 구현하기1) low-levelimport torchimport torch.nn.functional as Ftorch.manual_seed(1)# 3 x 5 크기의 텐서 생성# 5개의 클래스를 갖는 3개의 샘플z = torch.rand(3, 5, requires_grad=True)# 소프트맥스 적용hypothesis = F.softmax(z, dim=1)# 임의의 레이블 생성y = torch.randint(5, (3,)).long()# 모든 원소가 0의 값을 가진 3 x 5 텐서 생성y_one_hot = torch.zeros_like(hypothesis)# y.unsqueeze(1)를 하면 (3,)의 크기였던 y가 (3x1)텐서가 됨 : tensor([[0], [2], [1]])# scatter 첫번째 인자: dim=1 에 대해 수행# scatter 세번째 인자: 두번째 인자가 알려주는 위치에 숫자 1# 연산 뒤에 _가 붙은 경우 덮어쓰기 함y_one_hot.scatter_(1, y.unsqueeze(1), 1)# 결과# tensor([[1., 0., 0., 0., 0.],# [0., 0., 1., 0., 0.],# [0., 1., 0., 0., 0.]])# 비용 함수cost = (y_one_hot * - torch.log(hypothesis)).sum(dim=1).mean()2) high-level# 1. low level 수식#hypothesis = F.softmax(z, dim=1)#cost = (y_one_hot * - torch.log(hypothesis)).sum(dim=1).mean()# 2. 축약cost = (y_one_hot * - torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()# 3. F.log_softmax#torch.log(F.softmax(z, dim=1)) 가 많이 사용되므로, F.log_softmax 로 제공됨cost = (y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()# 4. 잔여 기능이 포함된 함수cost = F.nll_loss(F.log_softmax(z, dim=1), y)# 5. 모든 기능이 포함된 함수cost = F.cross_entropy(z, y)4. MNIST 데이터 분류1. MNIST 숫자 0부터 9까지의 이미지로 구성된 손글씨 데이터셋 60,000개의 훈련데이터와 레이블, 10,000개의 테스트 데이터와 레이블로 구성됨 레이블은 0부터 9까지 총 10개 각 이미지는 28 x 28 픽셀# torchvision은 유명한 데이터셋들, 이미 구현된 유명 모델들, 일반적인 전처리 도구들을 포함하고 있음# 자연어처리는 torchtext 사용import torchimport torchvision.datasets as dsetsimport torchvision.transforms as transformsfrom torch.utils.data import DataLoaderimport torch.nn as nnimport matplotlib.pyplot as pltimport random# GPU 연산이 가능하다면 GPU 사용USE_CUDA = torch.cuda.is_available()device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")print(\"다음 기기로 학습합니다:\", device)# 랜덤시드 고정random.seed(777)torch.manual_seed(777)if device == 'cuda': torch.cuda.manual_seed_all(777)# hyperparameterstraining_epochs = 15batch_size = 100# MNIST dataset# root : MNIST를 다운로드 받을 경로# train : 학습에 사용할지 여부# transform : 파이토치 텐서로 변환# download : 데이터가 없을 때 다운로드 받을지 여부mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=True)mnist_test = dsets.MNIST(root='MNIST_data/', train=False, transform=transforms.ToTensor(), download=True)# 데이터 로드# drop_last : 배치 크기로 데이터를 나누었을 때, 나머지가 발생하면 버릴 지 여부# 나머지의 갯수가 너무 적은 경우 상대적으로 과대평가 될 수 있음data_loader = DataLoader(dataset=mnist_train, batch_size=batch_size, # 배치 크기는 100 shuffle=True, drop_last=True)# 모델 설계# input_dim = 28 x 28 = 784# output_dim = 10 : 10개의 클래스이므로# .to() 어떤 장치를 사용해서 연산할 지 지정linear = nn.Linear(784, 10, bias=True).to(device)# 비용 함수와 옵티마이저 정의criterion = nn.CrossEntropyLoss().to(device)optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)# torch.nn.functional.cross_entropy()# torch.nn.CrossEntropyLoss()# nn 에 포함된 기능은 클래스형, nn.functional에 포함된 기능은 함수형. 기능은 같음# 학습for epoch in range(training_epochs): # 앞서 training_epochs의 값은 15로 지정함. avg_cost = 0 total_batch = len(data_loader) for X, Y in data_loader: # 배치 크기가 100이므로 아래의 연산에서 X는 (100, 784)의 텐서가 된다. X = X.view(-1, 28 * 28).to(device) # 레이블은 원-핫 인코딩이 된 상태가 아니라 0 ~ 9의 정수. Y = Y.to(device) optimizer.zero_grad() hypothesis = linear(X) cost = criterion(hypothesis, Y) cost.backward() optimizer.step() avg_cost += cost / total_batch print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))print('Learning finished')# 모델 테스트with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다. X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device) Y_test = mnist_test.test_labels.to(device) prediction = linear(X_test) correct_prediction = torch.argmax(prediction, 1) == Y_test accuracy = correct_prediction.float().mean() print('Accuracy:', accuracy.item()) # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다 r = random.randint(0, len(mnist_test) - 1) X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device) Y_single_data = mnist_test.test_labels[r:r + 1].to(device) print('Label: ', Y_single_data.item()) single_prediction = linear(X_single_data) print('Prediction: ', torch.argmax(single_prediction, 1).item()) plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest') plt.show()[6] 인공 신경망1. 퍼셉트론 퍼셉트론 Perceptron 초기 형태의 인공 신경망 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘 - x :입력 값- W : Weight 가중치- y : 출력값 각 입력값이 가중치와 곱해져 뉴런에 보내지고, 뉴런에서는 입력값x가중치의 합이 입계치를 넘으면 1을 출력 즉, 다음과 같은 계단 함수 임계치를 포함한 퍼셉트론2. 단층 퍼셉트론 입력층(input layer), 출력층(output layer) 두 단계로만 구성됨 논리게이트 문제 def AND_gate(x1, x2): w1=0.5 w2=0.5 b=-0.7 result = x1*w1 + x2*w2 + b if result &lt;= 0: return 0 else: return 1def NAND_gate(x1, x2): w1=-0.5 w2=-0.5 b=0.7 result = x1*w1 + x2*w2 + b if result &lt;= 0: return 0 else: return 1def OR_gate(x1, x2): w1=0.6 w2=0.6 b=-0.5 result = x1*w1 + x2*w2 + b if result &lt;= 0: return 0 else: return 13. 다층 퍼셉트론 MLP MultiLayer Perceptron XOR 게이트는 AND, NAND, OR 게이트를 조합하면 만들 수 있음. 즉, layer를 더 쌓으면 만들 수 있음 이와 같이 입력층과 출력층 사이에 존재하는 것을 은닉층 hidden layer 라 함import torchimport torch.nn as nndevice = 'cuda' if torch.cuda.is_available() else 'cpu'# for reproducibilitytorch.manual_seed(777)if device == 'cuda': torch.cuda.manual_seed_all(777)X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)model = nn.Sequential( nn.Linear(2, 10, bias=True), # input_layer = 2, hidden_layer1 = 10 nn.Sigmoid(), nn.Linear(10, 10, bias=True), # hidden_layer1 = 10, hidden_layer2 = 10 nn.Sigmoid(), nn.Linear(10, 10, bias=True), # hidden_layer2 = 10, hidden_layer3 = 10 nn.Sigmoid(), nn.Linear(10, 1, bias=True), # hidden_layer3 = 10, output_layer = 1 nn.Sigmoid() ).to(device)# 비용함수와 옵티마이저 선언# nn.BCELoss() 는 이진 분류에서 사용하는 크로스엔트로피 함수criterion = torch.nn.BCELoss().to(device)optimizer = torch.optim.SGD(model.parameters(), lr=1) # modified learning rate from 0.1 to 1# 학습for epoch in range(10001): optimizer.zero_grad() # forward 연산 hypothesis = model(X) # 비용 함수 cost = criterion(hypothesis, Y) cost.backward() optimizer.step() # 100의 배수에 해당되는 에포크마다 비용을 출력 if epoch % 100 == 0: print(epoch, cost.item())# 학습 결과 확인with torch.no_grad(): hypothesis = model(X) predicted = (hypothesis &gt; 0.5).float() accuracy = (predicted == Y).float().mean() print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy()) print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy()) print('실제값(Y): ', Y.cpu().numpy()) print('정확도(Accuracy): ', accuracy.item())" }, { "title": "PyTorch 입문", "url": "/posts/study-pytorch-beginner/", "categories": "Machine Learning, PyTorch", "tags": "study, Machine Learning, PyTorch, Beginner", "date": "2022-05-16 17:30:00 +0900", "snippet": "[0] 참고자료 PyTorch로 시작하는 딥 러닝 입문 유원준 외 1명 딥러닝 파이토치 교과서 서지영[1] 환경 설정(windows) 참고[2] 기초1. 데이터 타입 단위 값은 스칼라 Scalar 1차원 배열은 벡터 Vector 2차원은 행렬 Matrix 3차원 텐서 Tensor 상황에 따라 모두가 텐서가 될 수 있음. 1차원 텐서, 2차원 텐서 …2. 텐서 표기2.1 2D Tensor|t| = (Batch size, dimension) 3행 5열 = 3x5 Batch size = 3, dimension = 5-&gt; 하나의 데이터는 5개의 데이터로 구성되었으며, 한 번에 처리하는 훈련 데이터는 3개이다.2.2 3D Tensor|t| = (Batch size, width, height) 하나의 데이터는 width * height로 구성되었으며, 한 번에 처리하는 훈련 데이터는 Batch size만큼이다.3. 머신러닝 용어1) 데이터 셋 데이터셋 일반적으로 Training, Validation, Testing 세 개의 용도로 분리함 Validation 데이터는 모덱의 성능을 조정하기 위한 것 학습이 종료되면, Validation 데이터에 대해서도 일정부분 최적화가 되었으므로, 성능 테스트에 적합하지 않음 Training-문제지, Validation-모의고사, Testing-실전시험, 이후 real data 사용 하이퍼파라미터 모델의 성능에 영향을 주는 매개변수 모델의 성능을 조정하기 위한 용도 사용자가 직접 정해줄 수 있는 변수 ex) learning rate 파라미터 학습을 통해 바뀌어져가는 변수 2) Classification &amp; Regression머신러닝의 많은 문제가 여기에 속함 이진 분류 문제 (Binary Classification) 주어진 입력에 대해 둘 중 하나의 답을 정하는 문제 ex) Pass of Fail, 스팸여부 다중 클래스 분류 (Multi-class Classification) 여러 개의 선택지 중 하나의 답을 정하는 문제 ex) 꽃 품종 분류, 이물질 종류 분류 회귀 문제 (Regression) 연속된 값을 결과로 가짐 ex) 5시간 공부했을 때 80점, 5시간 1분 공부했을 때 80.5점, 7시간 공부했을 때 90점. 그러면 n시간 공부했을 때 점수는? 3) Supervised Learning &amp; Unsupervised Learning 지도 학습 Label이라는 정답과 함께 학습ㅎ는 것 비지도 학습 군집, 차원 축소와 같이 목적 데이터/레이블 없이 학습 하는 방법 강화 학습 어떤 환경에서 현재의 상태를 인식하여, 선택 가능한 행동들 중 보상을 최대화 하는 행동 또는 순서를 선택하는 방법 4. Sample &amp; Feature Sample : 하나의 데이터 Feature : y를 예측하기 위한 독립 변수 x5. Confusion Matrix 정확도 Accuracy 맞춘 문제 수 / 전체 문제 수 Confusion Matrix - 참 거짓 참 TP FN 거짓 FP TN - True Positive : Positive라 답하고 정답 - False Positive : Positive라 답하고 오답 - False Negative : Negative라 답하고 오답 - True Negative : Negative라 답하고 정답 정밀도 Precision 양성이라 답한 케이스에 대한 TP 비율 precision = TP / (TP + FP) 재현률 Recall 양성인 데이터를 실제로 얼마나 양성인지 예측, 즉 재현했는지 비율 Recall = TP / (TP + FN) F1-Score 정밀도와 재현률은 Trade-off 관계이므로, 적절한 지점을 찾기 위해 조화 평균을 사용함 2 * (Precision * Recall / (Precision + Recall)) TP / (2TP + FN + FP) 6. Overfitting &amp; Underfitting 과적합 Overfitting 훈련 데이터를 과하게 학습하여, 훈련데이터에만 적합해진 경우 과소적합 Underfitting 훈련을 덜 한 상태 [3] 선형 회귀 Linear Regression선형 회귀-&gt; x와 y의 선형 관례를 모델링하는 회귀분석 기법1. Hypothesis y = Wx + b H(x) = Wx + b 2. Cost Function 비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function) 예측 결과와 실제 결과의 차이를 구하는 함수 비용 함수가 최소값이 되게 하는 W와 b를 구하는 것이 목표3. Gradient Descent Cost Function의 값을 최소로 하는 W와 b를 찾을 때 사용하는 것이 Optimizer 알고리즘 Optimizer 알고리즘을 사용해서 W와 b를 찾는 과정을 training Gradient Descent는 기본적인 Optimizer 알고리즘 앞서 Cost Function은 W의 값이 너무 커지거나 너무 작아지면 그 결과값이 커진다. (2차원 방정식) 따라서 경사하강법은 cost function의 기울기가 0에 가까운 상태를 찾는다.2. 구현import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optim# 난수 발생 순서와 값을 동일하게 보장해 준다torch.manual_seed(1)x_train = torch.FloatTensor([[1], [2], [3]])y_train = torch.FloatTensor([[2], [4], [6]])W = torch.zeros(1, requires_grad=True) # size 1,1 인 Tensor 생성b = torch.zeros(1, requires_grad=True) # requires_grad=True 학습을 통해 변경되는 변수임optimizer = optim.SGD([W, b], lr=0.01)nb_epochs = 2000for epoch in range(nb_epochs + 1): # H(x) 계산 hypothesis = W * x_train + b # cost 계산. MSE - Mean Sqaure Error cost = torch.mean((hypothesis - y_train) ** 2) # cost 계산 # gradient를 0으로 초기화 optimizer.zero_grad() # 비용 함수를 미분하여 gradient 계산 cost.backward() # W와 b를 업데이트 optimizer.step() # 100번마다 로그 출력 if epoch % 100 == 0: print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), b.item(), cost.item()))다중 선형 회귀import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimtorch.manual_seed(1)x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])model = nn.Linear(3,1)print(list(model.parameters()))optimizer = optim.SGD(model.parameters(), lr=1e-5)nb_epochs = 2000for epoch in range(nb_epochs + 1): prediction = model(x_train) cost = F.mse_loss(prediction, y_train) optimizer.zero_grad() cost.backward() optimizer.step() if epoch % 100 == 0: print('Epoch {:4d}/{} Cost: {:.6f}'.format( epoch, nb_epochs, cost.item() ))new_var = torch.FloatTensor([[73, 80, 75]])pred_y = model(new_var)print('result : ', pred_y)[4] 로지스틱 회귀 Logistic Regression로지스틱 회귀 둘 중 하나를 결정하는 이진 분류(Binary Classification)을 풀기 위한 대표적인 알고리즘 회귀이지만 분류 작업에 사용함 S자 그래프가 그려지므로 새로운 가설이 필요 -&gt; Sigmoid function W의 값이 커지면 경사도가 커지고 bias의 값에 따라 그래프가 좌, 우 이동함구현import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimtorch.manual_seed(1)x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]y_data = [[0], [0], [0], [1], [1], [1]]x_train = torch.FloatTensor(x_data)y_train = torch.FloatTensor(y_data)model = nn.Sequential( nn.Linear(2, 1), nn.Sigmoid())optimizer = optim.SGD(model.parameters(), lr=1)nb_epochs = 1000for epoch in range(nb_epochs + 1): hypothesis = model(x_train) cost = F.binary_cross_entropy(hypothesis, y_train) optimizer.zero_grad() cost.backward() optimizer.step() if epoch % 20 == 0: prediction = hypothesis &gt;= torch.FloatTensor([0.5]) correct_prediction = prediction.float() == y_train accuracy = correct_prediction.sum().item() / len(correct_prediction) print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(epoch, nb_epochs, cost.item(), accuracy * 100))print(list(model.parameters()))" }, { "title": "블로그 포스팅 방법", "url": "/posts/how-to-post/", "categories": "Git", "tags": "tips, git, blog", "date": "2022-05-16 16:30:00 +0900", "snippet": "[1] 포스팅 포맷1. 최상단 포스트 정보 title: 포스트 제목 author: 작성자 ID _data/authors.yml에 등록된 사용자 정보가 매핑됩니다. 아이디는 일관성을 위해 git/slack ID로 임의로 등록해두었습니다. 그 외에 트위터 아이디나 개인 홈페이지를 연동 할 수 있습니다. date: 작성일자를 yyyy-MM-dd HH:mm:ss 형태로 양심껏 입력합니다. categories: [MAIN, SUB] 형태로 최대 2depth 카테고리가 가능합니다. 현재 포스트는 1depth만 갖고 있습니다. tags: [a, b, c, ..] 형태로 다수의 태그를 설정 할 수 있습니다. 현재 포스트는 tips, git, blog 3개의 태그가 설정되었습니다.2. 포스트 내용 Markdown 형태로 포스트를 작성하면 됩니다. 파일 _posts/tech/yyyy/MM/ 위치에 ‘yyyy-MM-dd-제목-띄어쓰기-없이.md 생성 tech가 아닌 다른 구분이 필요한 경우 추가할 수 있습니다. ex) playday 이미지 업로드 assets/img/post 위치에 포스트 디렉토리 구조와 같게 이미지 파일들을 위치시킵니다. ![이미지이름](/assets/img/post/이미지 path/이미지파일명) 형태로 포스트에 포함시킵니다. 3. md 파일 내용현재까지 이 포스트의 내용은 다음과 같습니다.---title: 블로그 포스팅 방법author: JaewukLee-Sdate: 2022-05-15 13:30:00 +0900categories: [Git]tags: [tips, git, blog]---## [1] 포스팅 포맷### 1. 최상단 포스트 정보- title: 포스트 제목- author: 작성자 ID - _data/authors.yml에 등록된 사용자 정보가 매핑됩니다. - 아이디는 일관성을 위해 git/slack ID로 임의로 등록해두었습니다. - 그 외에 트위터 아이디나 개인 홈페이지를 연동 할 수 있습니다.- date: 작성일자를 yyyy-MM-dd HH:mm:ss 형태로 양심껏 입력합니다.- categories: [MAIN, SUB] 형태로 최대 2depth 카테고리가 가능합니다. 현재 포스트는 1depth만 갖고 있습니다.- tags: [a, b, c, ..] 형태로 다수의 태그를 설정 할 수 있습니다. 현재 포스트는 tips, git, blog 3개의 태그가 설정되었습니다.### 2. 포스트 내용- Markdown 형태로 포스트를 작성하면 됩니다.- 파일 - _posts/tech/yyyy/MM/ 위치에 'yyyy-MM-dd-제목-띄어쓰기-없이.md 생성 - tech가 아닌 다른 구분이 필요한 경우 추가할 수 있습니다. ex) playday- 이미지 업로드 - assets/img/post 위치에 포스트 디렉토리 구조와 같게 이미지 파일들을 위치시킵니다. - ```![이미지이름](/assets/img/post/이미지 path/이미지파일명)``` 형태로 포스트에 포함시킵니다.![로고](/assets/img/kaier.png)### 3. md 파일 내용2. github.com에서 포스팅하기 github 접속 _posts/ 로 이동 Add files - Create new file로 포스트 작성 yyyy-MM-dd-제목.md 형태로 파일명 작성 포스트 내용 작성 포스트 커밋 : 커밋 메시지 입력 후 저장3. 로컬에서 포스팅하기IDE에서 md 미리보기를 지원하는 경우, 굳이 jekyll을 설치하지 않아도 됨 Ruby 설치 rubyinstaller.org/downloads/ jekyll 설치 gem install jekyll bundler 설치 gem install bundler clone github git clone https://github.com/kaiercorp/kaiercorp.github.io.git 실행 cd kaiercorp.github.io bundle exec jekyll serve localhost:4000/ 포스팅 _posts/ 위치에 포스팅 파일 작성 후, localhost:4000 에서 확인 한 뒤 커밋 " }, { "title": "Git Submodule 설정", "url": "/posts/git-submodule/", "categories": "Git", "tags": "tips, git, submodule", "date": "2022-05-12 13:30:00 +0900", "snippet": "1. 서브 모듈 처음 추가- 서브 모듈 추가~/workspace/parent_project$ git submodule add https://github.com/gitname/child_project 부모 프로젝트의 디렉토리에서 git submodule 실행 자식 프로젝트의 repository명과 동일한 디렉토리가 생성됨~/workspace/parent_projectgit submodule add https://github.com/gitname/child_project sub_project repository명인 child_project 대신 sub_project 라는 디렉토리에 서브모듈이 등록됨- 서브 모듈 추가 결과~/workspace/parent_project$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: .gitmodules new file: child_project .gitmodules : 서브모듈 목록 정보 child_project : 서브모듈 디렉토리 (서브 모듈 repository 명으로 생성됨)2. 서브 모듈이 있는 프로젝트 클론 부모 프로젝트 클론 서브 모듈 정보를 초기화 후 업데이트 ~/workspace/parent_project$ git sumbmodule init$ git submodule update " }, { "title": "VS code Anaconda 연동", "url": "/posts/vscode-anaconda/", "categories": "Anaconda", "tags": "install, win10, anaconda, python", "date": "2022-05-04 16:30:00 +0900", "snippet": "1. 확장 설치 VS code 실행 후 Extenstions 탭 클릭 Python 과 Code Runner 확장 설치 VS code 재실행2. 연동 ctrl + shift + p 누른 후 Python: Select Interpreter 선택 Python 환경 선택 ctrl + ` 눌러서 terminal 탭을 실행하면 자동으로 명령어를 입력하고 환경에 접속한 상태 확인 가능3. 에러 발생 시 ommandNotFoundError: Your shell has not been properly configured to use ‘conda activate’. 관련 " }, { "title": "Win 10 conda activate Error", "url": "/posts/conda-activate/", "categories": "Anaconda", "tags": "tips, error, vscode, win10", "date": "2022-05-04 16:11:00 +0900", "snippet": "문제 상황Anaconda 설치 및 가상 환경 생성 후 VS Code에서 가상환경 연동 시 에러 발생함.&gt; activate torch_book ommandNotFoundError: Your shell has not been properly configured to use ‘conda activate’. 환경변수 설정 되었음에도 에러 발생함원인powershell의 정책 설정이 안되어서 발생하는 에러처리PowerShell을 관리자 모드로 실행 후 다음 명령어 실행 set-ExecutionPolicy RemoteSigned" } ]
